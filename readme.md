# Small Datasets â€” Exploratory ML & DL Miniâ€‘Projects

> Handsâ€‘on, reproducible notebooks that demonstrate classic supervisedâ€‘learning workflows on *very* small tabular datasets. Perfect for showcasing endâ€‘toâ€‘end thinking without GPUâ€‘hungry data volumes.

---

## ğŸ“‚ Whatâ€™s Inside

| Notebook | Task                                                            | Library Stack                                     |
| -------- | --------------------------------------------------------------- | ------------------------------------------------- |
| ``       | Multiâ€‘class classification on the classic *Iris* flower dataset | `pandas`, `scikitâ€‘learn`, `matplotlib`, `seaborn` |
| ``       | Fishâ€‘species prediction on the Kaggle *Fish Market* dataset     | `pandas`, `PyTorch`, `scikitâ€‘learn`, `matplotlib` |

> **Why two notebooks?** They contrast a *traditional* ML workflow (scikitâ€‘learn pipelines) with a *deepâ€‘learning* pipeline (custom PyTorch loop) on similarly small, tidy datasetsâ€”highlighting tradeâ€‘offs in complexity, extensibility, and performance.

---

## ğŸš€ Quick Start

```bash
# 1ï¸âƒ£  Clone the repo
$ git clone https://github.com/aahiljivani/small_datasets.git && cd small_datasets

# 2ï¸âƒ£  Create a fresh environment (conda, mamba or venv)
$ python -m venv .venv && source .venv/bin/activate

# 3ï¸âƒ£  Install requirements (coming soon!)
$ pip install -r requirements.txt

# 4ï¸âƒ£  Launch Jupyter & run either notebook
$ jupyter lab
```

*Feel like hacking right away?* Open the notebook in **GoogleÂ Colab**â€”no local setup needed.

---

## ğŸ“ Project Summaries & Improvement Roadmap

### 1. *Iris* Classification (`iris_analysis.ipynb`)

**Current highlights**

- EDA with pairâ€‘plots & boxâ€‘plots
- Encoded labels, trained DecisionÂ Tree, RandomÂ Forest, and kâ€‘NN models
- Achieved â‰ˆÂ 97â€‘99â€¯% accuracy with `RandomForestClassifier`

**Areas for improvement**

1. **Pipeline consistency** â€“ wrap preprocessing + model in a single `Pipeline` for easier gridâ€‘search.
2. **Hyperâ€‘parameter tuning** â€“ run `GridSearchCV` or `Optuna` for fair model comparison.
3. **Crossâ€‘validation** â€“ replace train/test split with stratified 5â€‘fold CV.
4. **Model explainability** â€“ add `shap` or featureâ€‘importance plots.

---

### 2. Fishâ€‘Breed Classification with PyTorch (`Pytorch_simple_fish_breed_classification.ipynb`)

**Current highlights**

- Scaled 6 numeric features, built a 3â€‘layer MLP with `ReLU` activations.
- Tracked loss per epoch and generated a full `classification_report`.
- Demonstrated PyTorch basics (tensor ops, manual training loop).

**Areas for improvement**

1. **Class imbalance** â€“ use `WeightedRandomSampler` or classâ€‘weighted `CrossEntropyLoss` to rescue *Whitefish* precision/recall.
2. **Refactor into **``** & **`` â€“ enables batching, shuffling, and cleaner code reuse.
3. **Validation split & early stopping** â€“ prevent overâ€‘fitting on the tiny set.
4. **Baselines & storytelling** â€“ compare against logistic regression or gradientâ€‘boosting to justify DL.
5. **Repo hygiene** â€“ move model code to `src/`, keep notebooks for EDA/visualisation.

---

## ğŸ—ï¸ Suggested Repository Structure (nextÂ iteration)

```
small_datasets/
â”œâ”€â”€ data/                   # raw or processed CSVs (gitâ€‘ignored if large)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ iris_analysis.ipynb
â”‚   â””â”€â”€ fish_classification.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ iris_pipeline.py
â”‚   â””â”€â”€ fish_torch_train.py
â”œâ”€â”€ tests/                  # minimal pytest coverage
â”œâ”€â”€ requirements.txt        # pinned versions
â””â”€â”€ README.md
```

---

## ğŸ“ˆ TODOÂ List (openÂ issues welcome!)

-

---

## ğŸ¤ Contributing

Fork, branch off `main`, open a PR, and fill out the template. Even typo fixes are appreciated!

---

## ğŸ“œ License

TBDÂ â€” most likely MIT once requirements & CI are in place.

